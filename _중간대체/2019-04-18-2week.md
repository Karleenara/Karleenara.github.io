import tensorflow as tf
tf.enable_eager_execution()

#Data
x_data= [1,2,3,4,5]  #입력
y_data= [1,2,3,4,5]  #출력 x값과 y값 같음

#w,b initialize
w= tf.Variable(2.9)  # 임의의 초기값으로 2.9지정
b= tf.Variable(0.5)  # 임의의 초기값으로 0.5지정

learning_rate= 0.01  #grad값을 구했을 때 기울기를 얼만큼 반영할 것인가

for i in range(100+1): #100번 수행
    #Gradient descent
    with tf.GradientTape() as tape:   #변수들의 변화를 tape에 기록
        hypothesis = w * x_data + b   #가설 공식
        cost = tf.reduce_mean(tf.square(hypothesis - y_data)) #cost함수 공식 #reduce_mean() 차원줄임 #square() 제곱
    w_grad, b_grad = tape.gradient(cost, [w,b]) #tape을 호출하여 경사도 값을 구함. w,b에 대한 미분값을 구하여 각각 grad에 반환한다.  
    w.assign_sub(learning_rate * w_grad)  #A.assingn_sub(B) A= A-B  w update 
    b.assign_sub(learning_rate * b_grad)  #assingn함수 호출하여 b update
    if i%10 == 0:  #10의 배수가 될 때 마다 w,b값 출력
        print("{:5}|{:10.4}|{:10.4}|{:10.6f}".format(i,w.numpy(), b.numpy(),cost))
        
        #결과
        #w값 1.00으로 수렴. b값 0으로 수렴.
        #cost값 0에 가까운 값 출력. 오차가 적다. 실제값을 예측하는데 정확하다.
